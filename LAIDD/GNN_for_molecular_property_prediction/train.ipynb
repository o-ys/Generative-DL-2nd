{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbbd0c6c-951d-49c0-8224-321ed6cb554d",
   "metadata": {},
   "source": [
    "### 원본"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac3dd35-ab7e-4422-9f29-ff2671084161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from libs.io_utils import get_dataset\n",
    "from libs.io_utils import MyDataset\n",
    "from libs.io_utils import my_collate_fn\n",
    "\n",
    "from libs.models import MyModel\n",
    "\n",
    "from libs.utils import str2bool\n",
    "from libs.utils import set_seed\n",
    "from libs.utils import set_device\n",
    "from libs.utils import evaluate_classification\n",
    "\n",
    "\n",
    "def main(args):\n",
    "\t# Set random seeds and device\n",
    "\tset_seed(seed=args.seed)\n",
    "\tdevice = set_device(\n",
    "\t\tuse_gpu=args.use_gpu,\n",
    "\t\tgpu_idx=args.gpu_idx\n",
    "\t)\n",
    "\n",
    "\t# Prepare datasets and dataloaders\n",
    "\ttrain_set, valid_set, test_set = get_dataset(\n",
    "\t\tname=args.dataset_name,\n",
    "\t\tmethod=args.split_method,\n",
    "\t\tdata_seed=args.data_seed,\n",
    "\t)\n",
    "\t\n",
    "\ttrain_ds = MyDataset(splitted_set=train_set)\n",
    "\tvalid_ds = MyDataset(splitted_set=valid_set)\n",
    "\ttest_ds = MyDataset(splitted_set=test_set)\n",
    "\n",
    "\ttrain_loader = DataLoader(\n",
    "\t\tdataset=train_ds,\n",
    "\t\tbatch_size=args.batch_size,\n",
    "\t\tshuffle=True,\n",
    "\t\tnum_workers=args.num_workers,\n",
    "\t\tcollate_fn=my_collate_fn\n",
    "\t)\n",
    "\tvalid_loader = DataLoader(\n",
    "\t\tdataset=valid_ds,\n",
    "\t\tbatch_size=args.batch_size,\n",
    "\t\tshuffle=False,\n",
    "\t\tnum_workers=args.num_workers,\n",
    "\t\tcollate_fn=my_collate_fn\n",
    "\t)\n",
    "\ttest_loader = DataLoader(\n",
    "\t\tdataset=test_ds,\n",
    "\t\tbatch_size=args.batch_size,\n",
    "\t\tshuffle=False,\n",
    "\t\tnum_workers=args.num_workers,\n",
    "\t\tcollate_fn=my_collate_fn\n",
    "\t)\n",
    "\n",
    "\t# Construct model and load trained parameters if it is possible\n",
    "\tmodel = MyModel(\n",
    "\t\tmodel_type=args.model_type,\n",
    "\t\tnum_layers=args.num_layers,\n",
    "\t\thidden_dim=args.hidden_dim,\n",
    "\t\treadout=args.readout,\n",
    "\t\tdropout_prob=args.dropout_prob,\n",
    "\t\tis_classification=True,\n",
    "\t)\n",
    "\tmodel = model.to(device)\n",
    "\toptimizer = torch.optim.AdamW(\n",
    "\t\tmodel.parameters(), \n",
    "\t\tlr=args.lr,\n",
    "\t\tweight_decay=args.weight_decay,\n",
    "\t)\n",
    "\t'''\n",
    "\tscheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "\t\toptimizer=optimizer,\n",
    "\t\tT_0=args.num_epoches,\n",
    "\t\teta_min=1e-4,\n",
    "\t)\n",
    "\t'''\n",
    "\tscheduler = torch.optim.lr_scheduler.StepLR(\n",
    "\t\toptimizer=optimizer,\n",
    "\t\tstep_size=40,\n",
    "\t\tgamma=0.1,\n",
    "\t)\n",
    "\tbce_loss = nn.BCEWithLogitsLoss()\n",
    "\tfor epoch in range(args.num_epoches):\n",
    "\t\t# Train\n",
    "\t\tmodel.train()\n",
    "\t\tnum_batches = len(train_loader)\n",
    "\t\ttrain_loss = 0\n",
    "\t\ty_list = []\n",
    "\t\tpred_list = []\n",
    "\t\tfor i, batch in enumerate(train_loader):\n",
    "\t\t\tst = time.time()\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t\tgraph, y = batch[0], batch[1]\n",
    "\t\t\tgraph = graph.to(device)\n",
    "\t\t\ty = y.to(device)\n",
    "\t\t\ty = y.float()\n",
    "\n",
    "\t\t\tpred = model(graph).squeeze()\n",
    "\t\t\ty_list.append(y)\n",
    "\t\t\tpred_list.append(pred)\n",
    "\n",
    "\t\t\tloss = bce_loss(pred, y)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\ttrain_loss += loss.detach().cpu().numpy()\n",
    "\n",
    "\t\t\tet = time.time()\n",
    "\t\t\tprint (\"Train!!! Epoch:\", epoch+1, \\\n",
    "\t\t\t\t   \"\\t Batch:\", i+1, '/', num_batches, \\\n",
    "\t\t\t\t   \"\\t Loss:\", loss.detach().cpu().numpy(), \\\n",
    "\t\t\t\t   \"\\t Time spent:\", round(et-st, 2), \"(s)\")\n",
    "\t\tscheduler.step()\n",
    "\t\ttrain_loss /= num_batches\n",
    "\t\ttrain_metrics = evaluate_classification(\n",
    "\t\t\ty_list=y_list,\n",
    "\t\t\tpred_list=pred_list\n",
    "\t\t)\n",
    "\n",
    "\t\tmodel.eval()\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\t# Validation\n",
    "\t\t\tvalid_loss = 0\n",
    "\t\t\tnum_batches = len(valid_loader)\n",
    "\t\t\ty_list = []\n",
    "\t\t\tpred_list = []\n",
    "\t\t\tfor i, batch in enumerate(valid_loader):\n",
    "\t\t\t\tst = time.time()\n",
    "\n",
    "\t\t\t\tgraph, y = batch[0], batch[1]\n",
    "\t\t\t\tgraph = graph.to(device)\n",
    "\t\t\t\ty = y.to(device)\n",
    "\t\t\t\ty = y.float()\n",
    "\n",
    "\t\t\t\tpred = model(graph).squeeze()\n",
    "\t\t\t\ty_list.append(y)\n",
    "\t\t\t\tpred_list.append(pred)\n",
    "\n",
    "\t\t\t\tloss = bce_loss(pred, y)\n",
    "\t\t\t\tvalid_loss += loss.detach().cpu().numpy()\n",
    "\n",
    "\t\t\t\tet = time.time()\n",
    "\t\t\t\tprint (\"Valid!!! Epoch:\", epoch+1, \\\n",
    "\t\t\t\t\t   \"\\t Batch:\", i+1, '/', num_batches, \\\n",
    "\t\t\t\t\t   \"\\t Loss:\", loss.detach().cpu().numpy(), \\\n",
    "\t\t\t\t   \t   \"\\t Time spent:\", round(et-st, 2), \"(s)\")\n",
    "\t\t\tvalid_loss /= num_batches\n",
    "\t\t\tvalid_metrics = evaluate_classification(\n",
    "\t\t\t\ty_list=y_list,\n",
    "\t\t\t\tpred_list=pred_list\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\t# Test\n",
    "\t\t\ttest_loss = 0\n",
    "\t\t\tnum_batches = len(test_loader)\n",
    "\t\t\ty_list = []\n",
    "\t\t\tpred_list = []\n",
    "\t\t\tfor i, batch in enumerate(test_loader):\n",
    "\t\t\t\tst = time.time()\n",
    "\n",
    "\t\t\t\tgraph, y = batch[0], batch[1]\n",
    "\t\t\t\tgraph = graph.to(device)\n",
    "\t\t\t\ty = y.to(device)\n",
    "\t\t\t\ty = y.float()\n",
    "\n",
    "\t\t\t\tpred = model(graph).squeeze()\n",
    "\t\t\t\ty_list.append(y)\n",
    "\t\t\t\tpred_list.append(pred)\n",
    "\n",
    "\t\t\t\tloss = bce_loss(pred, y)\n",
    "\t\t\t\ttest_loss += loss.detach().cpu().numpy()\n",
    "\n",
    "\t\t\t\tet = time.time()\n",
    "\t\t\t\tprint (\"Test!!! Epoch:\", epoch+1, \\\n",
    "\t\t\t\t\t   \"\\t Batch:\", i+1, '/', num_batches, \\\n",
    "\t\t\t\t\t   \"\\t Loss:\", loss.detach().cpu().numpy(), \\\n",
    "\t\t\t\t   \t   \"\\t Time spent:\", round(et-st, 2), \"(s)\")\n",
    "\t\t\ttest_loss /= num_batches\n",
    "\t\t\ttest_metrics = evaluate_classification(\n",
    "\t\t\t\ty_list=y_list,\n",
    "\t\t\t\tpred_list=pred_list\n",
    "\t\t\t)\n",
    "\t\tprint (\"End of \", epoch+1, \"-th epoch\", \\\n",
    "\t\t\t   \"Accuracy:\", round(train_metrics[0], 3), \"\\t\", round(valid_metrics[0], 3), \"\\t\", round(test_metrics[0], 3), \\\n",
    "\t\t\t   \"AUROC:\", round(train_metrics[1], 3), \"\\t\", round(valid_metrics[1], 3), \"\\t\", round(test_metrics[1], 3), \\\n",
    "\t\t\t   \"Precision:\", round(train_metrics[2], 3), \"\\t\", round(valid_metrics[2], 3), \"\\t\", round(test_metrics[2], 3), \\\n",
    "\t\t\t   \"Recall:\", round(train_metrics[3], 3), \"\\t\", round(valid_metrics[3], 3), \"\\t\", round(test_metrics[3], 3), \\\n",
    "\t\t\t   \"F1-score:\", round(train_metrics[4], 3), \"\\t\", round(valid_metrics[4], 3), \"\\t\", round(test_metrics[4], 3), \\\n",
    "\t\t\t   \"ECE:\", round(train_metrics[5], 3), \"\\t\", round(valid_metrics[5], 3), \"\\t\", round(test_metrics[5], 3))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tparser = argparse.ArgumentParser()\n",
    "\tparser.add_argument('--job_title', type=str, default='Test', \n",
    "\t\t\t\t\t\thelp='Job title of this execution')\n",
    "\tparser.add_argument('--use_gpu', type=str2bool, default=True, \n",
    "\t\t\t\t\t\thelp='whether to use GPU device')\n",
    "\tparser.add_argument('--gpu_idx', type=str, default='1', \n",
    "\t\t\t\t\t\thelp='index of gpu to use')\n",
    "\tparser.add_argument('--seed', type=int, default=999,\n",
    "\t\t\t\t\t\thelp='Seed for all stochastic components')\n",
    "\n",
    "\tparser.add_argument('--dataset_name', type=str, default='BBBP', \n",
    "\t\t\t\t\t\thelp='What dataset to use for model development')\n",
    "\tparser.add_argument('--split_method', type=str, default='random', \n",
    "\t\t\t\t\t\thelp='How to split dataset')\n",
    "\tparser.add_argument('--data_seed', type=int, default=999,\n",
    "\t\t\t\t\t\thelp='Seed for dataset splitting')\n",
    "\n",
    "\tparser.add_argument('--model_type', type=str, default='gcn', \n",
    "\t\t\t\t\t\thelp='Type of GNN model, Options: gcn, gin, gin_e, gat, ggnn')\n",
    "\tparser.add_argument('--num_layers', type=int, default=3,\n",
    "\t\t\t\t\t\thelp='Number of GIN layers for ligand featurization')\n",
    "\tparser.add_argument('--hidden_dim', type=int, default=64,\n",
    "\t\t\t\t\t\thelp='Dimension of hidden features')\n",
    "\tparser.add_argument('--readout', type=str, default='sum', \n",
    "\t\t\t\t\t\thelp='Readout method, Options: sum, mean, ...')\n",
    "\tparser.add_argument('--dropout_prob', type=float, default=0.0, \n",
    "\t\t\t\t\t\thelp='Probability of dropout on node features')\n",
    "\n",
    "\tparser.add_argument('--optimizer', type=str, default='adam', \n",
    "\t\t\t\t\t\thelp='Options: adam, sgd, ...')\n",
    "\tparser.add_argument('--num_epoches', type=int, default=150,\n",
    "\t\t\t\t\t\thelp='Number of training epoches')\n",
    "\tparser.add_argument('--num_workers', type=int, default=8,\n",
    "\t\t\t\t\t\thelp='Number of workers to run dataloaders')\n",
    "\tparser.add_argument('--batch_size', type=int, default=64,\n",
    "\t\t\t\t\t\thelp='Number of samples in a single batch')\n",
    "\tparser.add_argument('--lr', type=float, default=1e-3, \n",
    "\t\t\t\t\t\thelp='Initial learning rate')\n",
    "\tparser.add_argument('--weight_decay', type=float, default=1e-6, \n",
    "\t\t\t\t\t\thelp='Weight decay coefficient')\n",
    "\n",
    "\tparser.add_argument('--save_model', type=str2bool, default=True, \n",
    "\t\t\t\t\t\thelp='whether to save model')\n",
    "\n",
    "\targs = parser.parse_args()\n",
    "\n",
    "\tprint (\"Arguments\")\n",
    "\tfor k, v in vars(args).items():\n",
    "\t\tprint (k, \": \", v)\n",
    "\tmain(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b331df-49af-4f0b-aa2b-12f858ebb0a9",
   "metadata": {},
   "source": [
    "### 주석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43b60aa-9a48-491c-9686-3d75e5002088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from libs.io_utils import get_dataset\n",
    "from libs.io_utils import MyDataset\n",
    "from libs.io_utils import my_collate_fn\n",
    "\n",
    "#libs패키지의 models.py모듈의 MyModel클래스를 import\n",
    "from libs.models import MyModel\n",
    "\n",
    "from libs.utils import str2bool\n",
    "from libs.utils import set_seed\n",
    "from libs.utils import set_device\n",
    "from libs.utils import evaluate_classification\n",
    "\n",
    "\n",
    "def main(args):\n",
    "\t### Set random seeds and device\n",
    "\tset_seed(seed=args.seed)\n",
    "\tdevice = set_device(\n",
    "\t\tuse_gpu=args.use_gpu,\n",
    "\t\tgpu_idx=args.gpu_idx\n",
    "\t)\n",
    "\n",
    "\t### Prepare datasets and dataloaders (데이터셋을 준비)\n",
    "    train_set, valid_set, test_set = get_dataset(\n",
    "        name=args.dataset_name,\n",
    "        method=args.split_method,\n",
    "        data_seed=args.data_seed,\n",
    "    )\n",
    "        \n",
    "\t# 각각 torch의 데이터셋으로 만듦. \n",
    "    # MyDataset의 return 값 => SMILES, label\n",
    "    train_ds = MyDataset(splitted_set=train_set)\n",
    "    valid_ds = MyDataset(splitted_set=valid_set)\n",
    "    test_ds = MyDataset(splitted_set=test_set)\n",
    "\n",
    "\t# 각 데이터셋을 데이터로더(데이터로더는 데이터셋에 접근하는 역할을 수행)에 넣어줘서 각각의 데이터로더로 만듦\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_ds,\n",
    "        # 한 batch에 들어있는 data들의 개수 = batch_size (예, idx가 32개면 batch_size가 32개)\n",
    "        batch_size=args.batch_size,\n",
    "        # idx들이 random sampling\n",
    "        # (보통 train에서는 shuffle=True <--- minibatch optimization (average)할 때 최대한 서로 다른 데이터 환경을 보기 때문에 / validation이나 test에서는 shuffle=False <--- shuffling을 안해도 결과가 똑같이 나오니까)\n",
    "        shuffle=True,\n",
    "        # batch를 뽑을 때, cpu 쓰레드 개수를 몇개 쓸건 지\n",
    "        num_workers=args.num_workers,\n",
    "        # 우리는 MyDataset으로 SMILES, label을 뽑는데 우리가 원하는 것은 graph data이니 Dataloader에서 SMILE, label을 변환해주는 함수가 collate_fn\n",
    "        collate_fn=my_collate_fn\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        dataset=valid_ds,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=args.num_workers,\n",
    "        collate_fn=my_collate_fn\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_ds,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=args.num_workers,\n",
    "        collate_fn=my_collate_fn\n",
    "    )\n",
    "\n",
    "\n",
    "    # Construct model and load trained parameters if it is possible\n",
    "    #내가 만든 libs 패키지의 mymodel.py모듈의 MyModel 클래스의 인스턴스(model) 생성\n",
    "    #model_type에 GCN,GIN 등이 될 수 있음\n",
    "    model = MyModel(\n",
    "        model_type=args.model_type,\n",
    "        num_layers=args.num_layers,\n",
    "        hidden_dim=args.hidden_dim,\n",
    "        readout=args.readout,\n",
    "        dropout_prob=args.dropout_prob,\n",
    "        is_classification=True,\n",
    "    )\n",
    "    #이 model을 gpu device에 보냄\n",
    "    model = model.to(device)\n",
    "    #AdamW(Adam optimzer used with weight decay) 옵티마이저 사용. Adam옵티마이저와 거의 같은데 weight decay관련 Adam 옵티마이저의 버그를 조금 수정한 것.\n",
    "    #이 옵티마이저로 loss를 back propagation 하여 parameter들을 업데이트할 예정.\n",
    "    optimizer = torch.optim.AdamW(\n",
    "\t\tmodel.parameters(), \n",
    "\t\tlr=args.lr,\n",
    "\t\tweight_decay=args.weight_decay,\n",
    "\t)\n",
    "    # 스케쥴러를 cosin annealing을 쓸수도 있지만\n",
    "\t'''\n",
    "\tscheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "\t\toptimizer=optimizer,\n",
    "\t\tT_0=args.num_epoches,\n",
    "\t\teta_min=1e-4,\n",
    "\t)\n",
    "\t'''\n",
    "    # 그냥 심플하게 stepLR 스케쥴러를을 씀\n",
    "\tscheduler = torch.optim.lr_scheduler.StepLR(\n",
    "\t\toptimizer=optimizer,\n",
    "\t\tstep_size=40,\n",
    "\t\tgamma=0.1,\n",
    "\t)\n",
    "    #binary classification loss를 선언\n",
    "\tbce_loss = nn.BCEWithLogitsLoss()\n",
    "\tfor epoch in range(args.num_epoches):\n",
    "        ### Train 파트\n",
    "\t\tmodel.train()\n",
    "        # num_batches: 전체 데이터 사이즈 / 배치사이즈\n",
    "\t\tnum_batches = len(train_loader)\n",
    "\t\ttrain_loss = 0\n",
    "\t\ty_list = []\n",
    "\t\tpred_list = []\n",
    "        # train loader를 for문 돌려가면서 batch들을 뱉고 \n",
    "\t\tfor i, batch in enumerate(train_loader):\n",
    "            st = time.time()\n",
    "            # 각 한 batch에 대해서 옵티마이저의 gradient를 0으로 초기화\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # batch에서 0번째는 graph, 첫번째는 y값\n",
    "            graph, y = batch[0], batch[1]\n",
    "\t\t\tgraph = graph.to(device)\n",
    "\t\t\ty = y.to(device)\n",
    "\t\t\ty = y.float()\n",
    "            \n",
    "            #model에 graph를 넣어서 prediction값을 얻음\n",
    "            #pred = model(graph, training=True).squeeze() 하면 dropout이 켜짐.\n",
    "            pred = model(graph).squeeze()\n",
    "            y_list.append(y)\n",
    "            pred_list.append(pred)\n",
    "\n",
    "            #prediction값과 y값을 비교해서 binary cross entropy loss를 구해서 \n",
    "            loss = bce_loss(pred, y)\n",
    "            #loss를 backward 해서 gradient를 계산하고 \n",
    "            loss.backward()\n",
    "            #이 gradient 나온 것을 optimzer.step()하면 back propagation해서 weight가 업데이트 됨\n",
    "            optimizer.step()\n",
    "\n",
    "            # loss들을 더해서 출력\n",
    "            train_loss += loss.detach().cpu().numpy()\n",
    "\n",
    "\t\t\tet = time.time()\n",
    "\t\t\tprint (\"Train!!! Epoch:\", epoch+1, \\\n",
    "\t\t\t\t   \"\\t Batch:\", i+1, '/', num_batches, \\\n",
    "\t\t\t\t   \"\\t Loss:\", loss.detach().cpu().numpy(), \\\n",
    "\t\t\t\t   \"\\t Time spent:\", round(et-st, 2), \"(s)\")\n",
    "        #learning rate 스케줄러를 업데이트   \n",
    "\t\tscheduler.step()\n",
    "\t\ttrain_loss /= num_batches\n",
    "        # 예측된 값을 바탕으로 train metric들을 구함\n",
    "\t\ttrain_metrics = evaluate_classification(\n",
    "\t\t\ty_list=y_list,\n",
    "\t\t\tpred_list=pred_list\n",
    "\t\t)\n",
    "\n",
    "\t\tmodel.eval()\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\t### Validation 파트. (Train 파트와 마찬가지로 loss와 metrics 구함)\n",
    "\t\t\tvalid_loss = 0\n",
    "\t\t\tnum_batches = len(valid_loader)\n",
    "\t\t\ty_list = []\n",
    "\t\t\tpred_list = []\n",
    "\t\t\tfor i, batch in enumerate(valid_loader):\n",
    "\t\t\t\tst = time.time()\n",
    "\n",
    "\t\t\t\tgraph, y = batch[0], batch[1]\n",
    "\t\t\t\tgraph = graph.to(device)\n",
    "\t\t\t\ty = y.to(device)\n",
    "\t\t\t\ty = y.float()\n",
    "\n",
    "\t\t\t\tpred = model(graph).squeeze()\n",
    "\t\t\t\ty_list.append(y)\n",
    "\t\t\t\tpred_list.append(pred)\n",
    "\n",
    "\t\t\t\tloss = bce_loss(pred, y)\n",
    "\t\t\t\tvalid_loss += loss.detach().cpu().numpy()\n",
    "\n",
    "\t\t\t\tet = time.time()\n",
    "\t\t\t\tprint (\"Valid!!! Epoch:\", epoch+1, \\\n",
    "\t\t\t\t\t   \"\\t Batch:\", i+1, '/', num_batches, \\\n",
    "\t\t\t\t\t   \"\\t Loss:\", loss.detach().cpu().numpy(), \\\n",
    "\t\t\t\t   \t   \"\\t Time spent:\", round(et-st, 2), \"(s)\")\n",
    "\t\t\tvalid_loss /= num_batches\n",
    "\t\t\tvalid_metrics = evaluate_classification(\n",
    "\t\t\t\ty_list=y_list,\n",
    "\t\t\t\tpred_list=pred_list\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\t### Test 파트. (Train 파트와 마찬가지로 loss와 metrics 구함)\n",
    "\t\t\ttest_loss = 0\n",
    "\t\t\tnum_batches = len(test_loader)\n",
    "\t\t\ty_list = []\n",
    "\t\t\tpred_list = []\n",
    "\t\t\tfor i, batch in enumerate(test_loader):\n",
    "\t\t\t\tst = time.time()\n",
    "\n",
    "\t\t\t\tgraph, y = batch[0], batch[1]\n",
    "\t\t\t\tgraph = graph.to(device)\n",
    "\t\t\t\ty = y.to(device)\n",
    "\t\t\t\ty = y.float()\n",
    "\n",
    "\t\t\t\tpred = model(graph).squeeze()\n",
    "\t\t\t\ty_list.append(y)\n",
    "\t\t\t\tpred_list.append(pred)\n",
    "\n",
    "\t\t\t\tloss = bce_loss(pred, y)\n",
    "\t\t\t\ttest_loss += loss.detach().cpu().numpy()\n",
    "\n",
    "\t\t\t\tet = time.time()\n",
    "\t\t\t\tprint (\"Test!!! Epoch:\", epoch+1, \\\n",
    "\t\t\t\t\t   \"\\t Batch:\", i+1, '/', num_batches, \\\n",
    "\t\t\t\t\t   \"\\t Loss:\", loss.detach().cpu().numpy(), \\\n",
    "\t\t\t\t   \t   \"\\t Time spent:\", round(et-st, 2), \"(s)\")\n",
    "\t\t\ttest_loss /= num_batches\n",
    "\t\t\ttest_metrics = evaluate_classification(\n",
    "\t\t\t\ty_list=y_list,\n",
    "\t\t\t\tpred_list=pred_list\n",
    "\t\t\t)\n",
    "        #각 epoch의 마지막에 Accuracy, AUROC, Precision, Recall, F1-score, ECE 를 출력\n",
    "        print (\"End of \", epoch+1, \"-th epoch\", \\\n",
    "\t\t\t   \"Accuracy:\", round(train_metrics[0], 3), \"\\t\", round(valid_metrics[0], 3), \"\\t\", round(test_metrics[0], 3), \\\n",
    "\t\t\t   \"AUROC:\", round(train_metrics[1], 3), \"\\t\", round(valid_metrics[1], 3), \"\\t\", round(test_metrics[1], 3), \\\n",
    "\t\t\t   \"Precision:\", round(train_metrics[2], 3), \"\\t\", round(valid_metrics[2], 3), \"\\t\", round(test_metrics[2], 3), \\\n",
    "\t\t\t   \"Recall:\", round(train_metrics[3], 3), \"\\t\", round(valid_metrics[3], 3), \"\\t\", round(test_metrics[3], 3), \\\n",
    "\t\t\t   \"F1-score:\", round(train_metrics[4], 3), \"\\t\", round(valid_metrics[4], 3), \"\\t\", round(test_metrics[4], 3), \\\n",
    "\t\t\t   \"ECE:\", round(train_metrics[5], 3), \"\\t\", round(valid_metrics[5], 3), \"\\t\", round(test_metrics[5], 3))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tparser = argparse.ArgumentParser()\n",
    "\tparser.add_argument('--job_title', type=str, default='Test', \n",
    "\t\t\t\t\t\thelp='Job title of this execution')\n",
    "\tparser.add_argument('--use_gpu', type=str2bool, default=True, \n",
    "\t\t\t\t\t\thelp='whether to use GPU device')\n",
    "\tparser.add_argument('--gpu_idx', type=str, default='1', \n",
    "\t\t\t\t\t\thelp='index of gpu to use')\n",
    "\tparser.add_argument('--seed', type=int, default=999,\n",
    "\t\t\t\t\t\thelp='Seed for all stochastic components')\n",
    "\n",
    "\tparser.add_argument('--dataset_name', type=str, default='BBBP', \n",
    "\t\t\t\t\t\thelp='What dataset to use for model development')\n",
    "\tparser.add_argument('--split_method', type=str, default='random', \n",
    "\t\t\t\t\t\thelp='How to split dataset')\n",
    "\tparser.add_argument('--data_seed', type=int, default=999,\n",
    "\t\t\t\t\t\thelp='Seed for dataset splitting')\n",
    "\n",
    "\tparser.add_argument('--model_type', type=str, default='gcn', \n",
    "\t\t\t\t\t\thelp='Type of GNN model, Options: gcn, gin, gin_e, gat, ggnn')\n",
    "\tparser.add_argument('--num_layers', type=int, default=3,\n",
    "\t\t\t\t\t\thelp='Number of GIN layers for ligand featurization')\n",
    "\tparser.add_argument('--hidden_dim', type=int, default=64,\n",
    "\t\t\t\t\t\thelp='Dimension of hidden features')\n",
    "\tparser.add_argument('--readout', type=str, default='sum', \n",
    "\t\t\t\t\t\thelp='Readout method, Options: sum, mean, ...')\n",
    "\tparser.add_argument('--dropout_prob', type=float, default=0.0, \n",
    "\t\t\t\t\t\thelp='Probability of dropout on node features')\n",
    "\n",
    "\tparser.add_argument('--optimizer', type=str, default='adam', \n",
    "\t\t\t\t\t\thelp='Options: adam, sgd, ...')\n",
    "\tparser.add_argument('--num_epoches', type=int, default=150,\n",
    "\t\t\t\t\t\thelp='Number of training epoches')\n",
    "\tparser.add_argument('--num_workers', type=int, default=8,\n",
    "\t\t\t\t\t\thelp='Number of workers to run dataloaders')\n",
    "\tparser.add_argument('--batch_size', type=int, default=64,\n",
    "\t\t\t\t\t\thelp='Number of samples in a single batch')\n",
    "\tparser.add_argument('--lr', type=float, default=1e-3, \n",
    "\t\t\t\t\t\thelp='Initial learning rate')\n",
    "\tparser.add_argument('--weight_decay', type=float, default=1e-6, \n",
    "\t\t\t\t\t\thelp='Weight decay coefficient')\n",
    "\n",
    "\tparser.add_argument('--save_model', type=str2bool, default=True, \n",
    "\t\t\t\t\t\thelp='whether to save model')\n",
    "\n",
    "\targs = parser.parse_args()\n",
    "\n",
    "\tprint (\"Arguments\")\n",
    "\tfor k, v in vars(args).items():\n",
    "\t\tprint (k, \": \", v)\n",
    "\tmain(args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
