{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05f28289-66e3-4756-91c0-d477c83c7ee9",
   "metadata": {},
   "source": [
    "# <font color=red>모델 구성(Forward)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521e08c2-c71b-45cb-9cd9-af88342fcd64",
   "metadata": {},
   "source": [
    "### 원본"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7908aa9-d945-43d1-8fae-3d1a00370f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import dgl\n",
    "\n",
    "from libs.layers import GraphConvolution\n",
    "from libs.layers import GraphIsomorphism\n",
    "from libs.layers import GraphIsomorphismEdge\n",
    "from libs.layers import GraphAttention\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "\tdef __init__(\n",
    "\t\t\tself, \n",
    "\t\t\tmodel_type,\n",
    "\t\t\tnum_layers=4, \n",
    "\t\t\thidden_dim=64,\n",
    "\t\t\tnum_heads=4, # Only used for GAT\n",
    "\t\t\tdropout_prob=0.2,\n",
    "\t\t\tbias_mlp=True,\n",
    "\t\t\treadout='sum',\n",
    "\t\t\tact=F.relu,\n",
    "\t\t\tinitial_node_dim=58,\n",
    "\t\t\tinitial_edge_dim=6,\n",
    "\t\t\tis_classification=False,\n",
    "\t\t):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.num_layers = num_layers\n",
    "\t\tself.embedding_node = nn.Linear(initial_node_dim, hidden_dim, bias=False)\n",
    "\t\tself.embedding_edge = nn.Linear(initial_edge_dim, hidden_dim, bias=False)\n",
    "\t\tself.readout = readout\n",
    "\n",
    "\t\tself.mp_layers = torch.nn.ModuleList()\n",
    "\t\tfor _ in range(self.num_layers):\n",
    "\t\t\tmp_layer = None\n",
    "\t\t\tif model_type == 'gcn':\n",
    "\t\t\t\tmp_layer = GraphConvolution(\n",
    "\t\t\t\t\thidden_dim=hidden_dim,\n",
    "\t\t\t\t\tdropout_prob=dropout_prob,\n",
    "\t\t\t\t\tact=act,\n",
    "\t\t\t\t)\n",
    "\t\t\telif model_type == 'gin':\n",
    "\t\t\t\tmp_layer = GraphIsomorphism(\n",
    "\t\t\t\t\thidden_dim=hidden_dim,\n",
    "\t\t\t\t\tdropout_prob=dropout_prob,\n",
    "\t\t\t\t\tact=act,\n",
    "\t\t\t\t\tbias_mlp=bias_mlp\n",
    "\t\t\t\t)\n",
    "\t\t\telif model_type == 'gin_e':\n",
    "\t\t\t\tmp_layer = GraphIsomorphismEdge(\n",
    "\t\t\t\t\thidden_dim=hidden_dim,\n",
    "\t\t\t\t\tdropout_prob=dropout_prob,\n",
    "\t\t\t\t\tact=act,\n",
    "\t\t\t\t\tbias_mlp=bias_mlp\n",
    "\t\t\t\t)\n",
    "\t\t\telif model_type == 'gat':\n",
    "\t\t\t\tmp_layer = GraphAttention(\n",
    "\t\t\t\t\thidden_dim=hidden_dim,\n",
    "\t\t\t\t\tnum_heads=num_heads,\n",
    "\t\t\t\t\tdropout_prob=dropout_prob,\n",
    "\t\t\t\t\tact=act,\n",
    "\t\t\t\t\tbias_mlp=bias_mlp\n",
    "\t\t\t\t)\n",
    "\t\t\telse:\n",
    "\t\t\t\traise ValueError('Invalid model type: you should choose model type in [gcn, gin, gin_e, gat, ggnn]')\n",
    "\t\t\tself.mp_layers.append(mp_layer)\n",
    "\n",
    "\t\tself.linear_out = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "\t\tself.is_classification = is_classification\n",
    "\t\tif self.is_classification:\n",
    "\t\t\tself.sigmoid = F.sigmoid\n",
    "\t\n",
    "\n",
    "\tdef forward(\n",
    "\t\t\tself, \n",
    "\t\t\tgraph,\n",
    "\t\t\ttraining=False,\n",
    "\t\t):\n",
    "\t\th = self.embedding_node(graph.ndata['h'].float())\n",
    "\t\te_ij = self.embedding_edge(graph.edata['e_ij'].float())\n",
    "\t\tgraph.ndata['h'] = h\n",
    "\t\tgraph.edata['e_ij'] = e_ij\n",
    "\n",
    "\t\t# Update the node features\n",
    "\t\tfor i in range(self.num_layers):\n",
    "\t\t\tgraph = self.mp_layers[i](\n",
    "\t\t\t\tgraph=graph,\n",
    "\t\t\t\ttraining=training\n",
    "\t\t\t)\n",
    "\n",
    "\t\t# Aggregate the node features and apply the last linear layer to compute the logit\n",
    "\t\tout = dgl.readout_nodes(graph, 'h', op=self.readout)\n",
    "\t\tout = self.linear_out(out)\n",
    "\n",
    "\t\tif self.is_classification:\n",
    "\t\t\tout = self.sigmoid(out)\n",
    "\t\treturn out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f868aa1-9c97-43f1-b69c-bff6c784b4e7",
   "metadata": {},
   "source": [
    "### 주석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12c1ef5a-fe35-4bc9-b7ed-29e50a01cb39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import dgl\n",
    "\n",
    "from layers import GraphConvolution\n",
    "from layers import GraphIsomorphism\n",
    "from layers import GraphIsomorphismEdge\n",
    "from layers import GraphAttention\n",
    "\n",
    "# 자식 클래스 MyModel 선언 (부모 클래스 torch.nn.Module를 상속 받음)\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            model_type,    # model type은 그때 그때 설정\n",
    "            num_layers=4,  # node (embedding?) layer: 4\n",
    "            hidden_dim=64, # hidden dimension: 64\n",
    "            num_heads=4,   ### Only used for GAT\n",
    "            dropout_prob=0.2,\n",
    "            bias_mlp=True, # MLP에 들어가는 bias 사용\n",
    "            readout='sum',\n",
    "            act=F.relu,    # non-linear activation: ReLU\n",
    "            initial_node_dim=58,  # 맨처음 node dimention: 58 (node 하나당 dimension)\n",
    "            initial_edge_dim=6,   # 맨처음 edge dimention: 6\n",
    "            is_classification=False,\n",
    "        ):\n",
    "\t\t # 부모클래스에 없는 다른 작업 추가\n",
    "        super().__init__()\n",
    "        \n",
    "\t\t### 정의 \n",
    "        self.num_layers = num_layers\n",
    "        # Linear layer(MLP)로 embedding: initial node feature의 dimension을 64로 맞춰주기 위해서 node feature를 embedding하는 layer 정의 (linear transformation)\n",
    "        # dimension만 바꾸면 되기 때문에 bias parameter를 사용하여 overfitting에 취약하게 만들 수 있어서 사용하지 않음. \n",
    "        self.embedding_node = nn.Linear(initial_node_dim, hidden_dim, bias=False)\n",
    "        self.embedding_edge = nn.Linear(initial_edge_dim, hidden_dim, bias=False)\n",
    "        # sum/ max/ mean readout할 지\n",
    "        self.readout = readout\n",
    "        \n",
    "\t\t# self.mp_layers 정의 후        \n",
    "        # message passing layer: 특정 node를 설명하기 위한 재료로 그 node의 neighborhood의 특징을 모으는 과정이 바로 Message Passing\n",
    "        # ModuleList(): 이것은 간단히 말해 nn.Module을 리스트로 정리하는 방법이다.\n",
    "        # 각 레이어를 리스트에 전달하고 레이어의 iterator를 만든다. 덕분에 forward처리를 간단하게 할 수 있다는 듯 하다.\n",
    "        # 밑에서 append해줄 mp_layer 하나하나가 torch.nn.Module 이라는 뜻.\n",
    "        self.mp_layers = torch.nn.ModuleList()\n",
    "        \n",
    "        # model_type(GCN or GIN or GIN_E or GAT) 중 하나를 num_layer=4번\n",
    "        for _ in range(self.num_layers):\n",
    "            mp_layer = None\n",
    "            if model_type == 'gcn':\n",
    "                mp_layer = GraphConvolution(\n",
    "                    hidden_dim=hidden_dim,\n",
    "                    dropout_prob=dropout_prob,\n",
    "                    act=act,\n",
    "                )\n",
    "            elif model_type == 'gin':\n",
    "                mp_layer = GraphIsomorphism(\n",
    "                    hidden_dim=hidden_dim,\n",
    "                    dropout_prob=dropout_prob,\n",
    "                    act=act,\n",
    "                    bias_mlp=bias_mlp\n",
    "                )\n",
    "            # GIN을 하는데 edge feature까지 함께쓰는 GIN_E 모델\n",
    "            elif model_type == 'gin_e':\n",
    "                mp_layer = GraphIsomorphismEdge(\n",
    "                    hidden_dim=hidden_dim,\n",
    "                    dropout_prob=dropout_prob,\n",
    "                    act=act,\n",
    "                    bias_mlp=bias_mlp\n",
    "                )\n",
    "            elif model_type == 'gat':\n",
    "                mp_layer = GraphAttention(\n",
    "                    hidden_dim=hidden_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    dropout_prob=dropout_prob,\n",
    "                    act=act,\n",
    "                    bias_mlp=bias_mlp\n",
    "                )\n",
    "            else:\n",
    "                # 예외처리: model_type을 잘 못 써줬을 경우\n",
    "                raise ValueError('Invalid model type: you should choose model type in [gcn, gin, gin_e, gat, ggnn]')\n",
    "                \n",
    "\t\t\t# self.mp_layer(nn.ModuleList)에 GCN(or GIN or GIN_E or GAT)을 num_layer=4번 넣음\n",
    "            self.mp_layers.append(mp_layer)\n",
    "\n",
    "        # linear transform으로 output이 하나의 스칼라 값이 나오도록 함\n",
    "        self.linear_out = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "        # is_classification: False\n",
    "        self.is_classification = is_classification\n",
    "        # classification이 True이면 sigmoid\n",
    "        if self.is_classification:\n",
    "            self.sigmoid = F.sigmoid \n",
    "    \n",
    "\n",
    "\t# forward() \"graph를 받았을 때 어떻게 실행할 건지. 위에서 정의한 것들을 조합.\" \n",
    "    # 그래프가 들어왔을 때 prediction 값을 구하는 forward pass 하는 부분의 함수\n",
    "    # training 하려면 true, training 안하려면 false (training=False라는 뜻은 dropout을 끄겠다는 의미)\n",
    "    def forward(\n",
    "            self, \n",
    "            graph,\n",
    "            training=False,\n",
    "        ):\n",
    "\t\t# graph의 initial node data (전체 node의 atom feature list)를 float형태로 변환 후 \n",
    "        # embedding_node(linear layer로 linear transformation)한 후 h로 바꾸고\n",
    "        h = self.embedding_node(graph.ndata['h'].float())\n",
    "        e_ij = self.embedding_edge(graph.edata['e_ij'].float())\n",
    "        # embedding한 node를 다시 graph의 node data로 넣어줌 \n",
    "        graph.ndata['h'] = h\n",
    "        graph.edata['e_ij'] = e_ij\n",
    "\n",
    "\t\t### Update the node features\n",
    "        # message passing layer에다가 num_layers=4번 node feature들을 업데이트(graph를 업데이트) 해주는 파트. \n",
    "        for i in range(self.num_layers):            \n",
    "            # 처음 graph가 ex)GCN(message passing layer)에 들어가서 업데이트 된 graph가 나오고, 그 graph가 또 GCN에 들어가서 graph가 업데이트 되는 방식. \n",
    "            # training=training : True\n",
    "            graph = self.mp_layers[i](\n",
    "                graph=graph,\n",
    "                training=training\n",
    "            )\n",
    "                \n",
    "\t\t### Aggregate the node features and apply the last linear layer to compute the logit\n",
    "        # 최종 업데이트된 graph에서 node feature들을 aggregation 해주고 'sum' aggregation으로 readout\n",
    "        out = dgl.readout_nodes(graph, 'h', op=self.readout)\n",
    "        # last linear layer를 통해서 graph를 하나의 스칼라값(:prediction 값)으로 업데이트\n",
    "        out = self.linear_out(out)\n",
    "\n",
    "        # classification이 True이면 sigmoid activaton function에 넣음 \n",
    "        if self.is_classification:\n",
    "            out = self.sigmoid(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cf71d2-5009-4bd1-9910-718c182f2044",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
