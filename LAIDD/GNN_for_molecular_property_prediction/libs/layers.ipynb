{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red>레이어 구성</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 원본"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "from dgl.nn.functional import edge_softmax\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\tdef __init__(\n",
    "\t\tself, \n",
    "\t\tinput_dim, \n",
    "\t\thidden_dim, \n",
    "\t\toutput_dim,\n",
    "\t\tbias=True,\n",
    "\t\tact=F.relu,\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.input_dim = input_dim\n",
    "\t\tself.hidden_dim = hidden_dim\n",
    "\t\tself.output_dim = output_dim\n",
    "\n",
    "\t\tself.act = act\n",
    "\n",
    "\t\tself.linear1 = nn.Linear(input_dim, hidden_dim, bias=bias)\n",
    "\t\tself.linear2 = nn.Linear(hidden_dim, output_dim, bias=bias)\n",
    "\t\n",
    "\tdef forward(self, h):\n",
    "\t\th = self.linear1(h)\n",
    "\t\th = self.act(h)\n",
    "\t\th = self.linear2(h)\n",
    "\t\treturn h\n",
    "\n",
    "\n",
    "class GraphConvolution(nn.Module):\n",
    "\tdef __init__(\n",
    "\t\t\tself,\n",
    "\t\t\thidden_dim,\n",
    "\t\t\tact=F.relu,\n",
    "\t\t\tdropout_prob=0.2,\n",
    "\t\t):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.act = act\n",
    "\t\tself.norm = nn.LayerNorm(hidden_dim)\n",
    "\t\tself.prob = dropout_prob\n",
    "\t\tself.linear = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "\t\n",
    "\tdef forward(\n",
    "\t\t\tself, \n",
    "\t\t\tgraph, \n",
    "\t\t\ttraining=False\n",
    "\t\t):\n",
    "\t\th0 = graph.ndata['h']\n",
    "\n",
    "\t\tgraph.update_all(fn.copy_u('h', 'm'), fn.sum('m', 'u_'))\n",
    "\t\th = self.act(self.linear(graph.ndata['u_'])) + h0\n",
    "\t\th = self.norm(h)\n",
    "\t\t\t\n",
    "\t\t# Apply dropout on node features\n",
    "\t\th = F.dropout(h, p=self.prob, training=training)\n",
    "\n",
    "\t\tgraph.ndata['h'] = h\n",
    "\t\treturn graph\n",
    "\n",
    "\n",
    "class GraphIsomorphism(nn.Module):\n",
    "\tdef __init__(\n",
    "\t\t\tself,\n",
    "\t\t\thidden_dim,\n",
    "\t\t\tact=F.relu,\n",
    "\t\t\tbias_mlp=True,\n",
    "\t\t\tdropout_prob=0.2,\n",
    "\t\t):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.mlp = MLP(\n",
    "\t\t\tinput_dim=hidden_dim,\n",
    "\t\t\thidden_dim=4*hidden_dim,\n",
    "\t\t\toutput_dim=hidden_dim,\n",
    "\t\t\tbias=bias_mlp,\n",
    "\t\t\tact=act\n",
    "\t\t)\n",
    "\t\tself.norm = nn.LayerNorm(hidden_dim)\n",
    "\t\tself.prob = dropout_prob\n",
    "\t\n",
    "\tdef forward(\n",
    "\t\t\tself, \n",
    "\t\t\tgraph, \n",
    "\t\t\ttraining=False\n",
    "\t\t):\n",
    "\t\th0 = graph.ndata['h']\n",
    "\n",
    "\t\tgraph.update_all(fn.copy_u('h', 'm'), fn.sum('m', 'u_'))\n",
    "\t\th = self.mlp(graph.ndata['u_']) + h0\n",
    "\t\th = self.norm(h)\n",
    "\t\t\t\n",
    "\t\t# Apply dropout on node features\n",
    "\t\th = F.dropout(h, p=self.prob, training=training)\n",
    "\n",
    "\t\tgraph.ndata['h'] = h\n",
    "\t\treturn graph\n",
    "\n",
    "\n",
    "class GraphIsomorphismEdge(nn.Module):\n",
    "\tdef __init__(\n",
    "\t\t\tself,\n",
    "\t\t\thidden_dim,\n",
    "\t\t\tact=F.relu,\n",
    "\t\t\tbias_mlp=True,\n",
    "\t\t\tdropout_prob=0.2,\n",
    "\t\t):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.norm = nn.LayerNorm(hidden_dim)\n",
    "\t\tself.prob = dropout_prob\n",
    "\t\tself.mlp = MLP(\n",
    "\t\t\tinput_dim=hidden_dim,\n",
    "\t\t\thidden_dim=4*hidden_dim,\n",
    "\t\t\toutput_dim=hidden_dim,\n",
    "\t\t\tbias=bias_mlp,\n",
    "\t\t\tact=act,\n",
    "\t\t)\n",
    "\t\n",
    "\tdef forward(\n",
    "\t\t\tself, \n",
    "\t\t\tgraph, \n",
    "\t\t\ttraining=False\n",
    "\t\t):\n",
    "\t\th0 = graph.ndata['h']\n",
    "\n",
    "\t\tgraph.update_all(fn.copy_u('h', 'm'), fn.sum('m', 'neigh'))\n",
    "\t\tgraph.update_all(fn.copy_edge('e_ij', 'm_e'), fn.sum('m_e', 'u_'))\n",
    "\t\tu_ = graph.ndata['neigh'] + graph.ndata['u_']\n",
    "\t\th = self.mlp(u_) + h0\n",
    "\t\th = self.norm(h)\n",
    "\t\t\t\n",
    "\t\t# Apply dropout on node features\n",
    "\t\th = F.dropout(h, p=self.prob, training=training)\n",
    "\n",
    "\t\tgraph.ndata['h'] = h\n",
    "\t\treturn graph\n",
    "\n",
    "\n",
    "class GraphAttention(nn.Module):\n",
    "\tdef __init__(\n",
    "\t\t\tself,\n",
    "\t\t\thidden_dim,\n",
    "\t\t\tnum_heads=4,\n",
    "\t\t\tbias_mlp=True,\n",
    "\t\t\tdropout_prob=0.2,\n",
    "\t\t\tact=F.relu,\n",
    "\t\t):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.mlp = MLP(\n",
    "\t\t\tinput_dim=hidden_dim,\n",
    "\t\t\thidden_dim=2*hidden_dim,\n",
    "\t\t\toutput_dim=hidden_dim,\n",
    "\t\t\tbias=bias_mlp,\n",
    "\t\t\tact=act,\n",
    "\t\t)\n",
    "\t\tself.hidden_dim = hidden_dim\n",
    "\t\tself.num_heads = num_heads\n",
    "\t\tself.splitted_dim = hidden_dim // num_heads\n",
    "\n",
    "\t\tself.prob = dropout_prob\n",
    "\n",
    "\t\tself.w1 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "\t\tself.w2 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "\t\tself.w3 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "\t\tself.w4 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "\t\tself.w5 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "\t\tself.w6 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "\n",
    "\t\tself.act = F.elu\n",
    "\t\tself.norm = nn.LayerNorm(hidden_dim)\n",
    "\t\n",
    "\tdef forward(\n",
    "\t\t\tself, \n",
    "\t\t\tgraph, \n",
    "\t\t\ttraining=False\n",
    "\t\t):\n",
    "\t\th0 = graph.ndata['h']\n",
    "\t\te_ij = graph.edata['e_ij']\n",
    "\n",
    "\t\tgraph.ndata['u'] = self.w1(h0).view(-1, self.num_heads, self.splitted_dim)\n",
    "\t\tgraph.ndata['v'] = self.w2(h0).view(-1, self.num_heads, self.splitted_dim)\n",
    "\t\tgraph.edata['x_ij'] = self.w3(e_ij).view(-1, self.num_heads, self.splitted_dim)\n",
    "\n",
    "\t\tgraph.apply_edges(fn.v_add_e('v', 'x_ij', 'm'))\n",
    "\t\tgraph.apply_edges(fn.u_mul_e('u', 'm', 'attn'))\n",
    "\t\tgraph.edata['attn'] = edge_softmax(graph, graph.edata['attn'] / math.sqrt(self.splitted_dim))\n",
    "\t\n",
    "\n",
    "\t\tgraph.ndata['k'] = self.w4(h0).view(-1, self.num_heads, self.splitted_dim)\n",
    "\t\tgraph.edata['x_ij'] = self.w5(e_ij).view(-1, self.num_heads, self.splitted_dim)\n",
    "\t\tgraph.apply_edges(fn.v_add_e('k', 'x_ij', 'm'))\n",
    "\n",
    "\t\tgraph.edata['m'] = graph.edata['attn'] * graph.edata['m']\n",
    "\t\tgraph.update_all(fn.copy_edge('m', 'm'), fn.sum('m', 'h'))\n",
    "\t\t\n",
    "\t\th = self.w6(h0) + graph.ndata['h'].view(-1, self.hidden_dim)\n",
    "\t\th = self.norm(h)\n",
    "\n",
    "\t\t# Add and Norm module\n",
    "\t\th = h + self.mlp(h)\n",
    "\t\th = self.norm(h)\n",
    "\t\t\t\n",
    "\t\t# Apply dropout on node features\n",
    "\t\th = F.dropout(h, p=self.prob, training=training)\n",
    "\n",
    "\t\tgraph.ndata['h'] = h \n",
    "\t\treturn graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "from dgl.nn.functional import edge_softmax\n",
    "\n",
    "\n",
    "# Multi Layer Perceptron (GIN, GAT에서 사용할 수 있음)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_dim, \n",
    "        hidden_dim, \n",
    "        output_dim,\n",
    "        bias=True,\n",
    "        act=F.relu,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.act = act\n",
    "\n",
    "        # bias=bias: True\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim, bias=bias)\n",
    "        self.linear2 = nn.Linear(hidden_dim, output_dim, bias=bias)\n",
    "    \n",
    "    def forward(self, h):\n",
    "        h = self.linear1(h)\n",
    "        h = self.act(h)\n",
    "        h = self.linear2(h)\n",
    "        return h\n",
    "\n",
    "# Graph Convolutional Layer\n",
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            hidden_dim,\n",
    "            act=F.relu,\n",
    "            dropout_prob=0.2,\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.act = act\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        self.prob = dropout_prob\n",
    "\t\t# linear transformation 하기위해 [Linear layer]선언 \n",
    "        self.linear = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "            \n",
    "    def forward(\n",
    "            self, \n",
    "            graph, \n",
    "            training=False\n",
    "        ):\n",
    "        # h0: 처음 graph의 각 node들의 data\n",
    "        h0 = graph.ndata['h']\n",
    "\n",
    "\t\t# 이론 PPT 87p 식:  σ(ΣHW)\n",
    "        # GCN은 주변에 있는 node feature들을 aggregation하는 것. \n",
    "        # ΣHW: node feature들(h)을 --copy--> m이라는 variable / m이라는 variable 들을 전부 --sum-->  u_라는 variable\n",
    "        graph.update_all(fn.copy_u('h', 'm'), fn.sum('m', 'u_'))\n",
    "\t\t# σ(ΣHW) + h0 (+ h0: residual connection)\n",
    "        # u_라는 variable이 graph.ndata에 있을 것이고 이것을 [linear layer]에 넣은 후 activation 한게 Graph Convolution연산\n",
    "        # 거기에다가 원래있던 node h0를 더하는 residual connection\n",
    "        h = self.act(self.linear(graph.ndata['u_'])) + h0\n",
    "        # layer normalization: optimization을 안정화하는데 도움이 됨\n",
    "        h = self.norm(h)\n",
    "        \t\n",
    "        ### Apply dropout on node features\n",
    "        # 업데이트한 node feature에 dropout을 쓸지 말지 (training이여서 dropout을 틀지) 결정\n",
    "        h = F.dropout(h, p=self.prob, training=training)\n",
    "\n",
    "        # 업데이트 된 node feature들 다시 graph.ndata에 업데이트하여 graph의 node feature들이 업데이트\n",
    "        graph.ndata['h'] = h\n",
    "        # 업데이트 된 graph 리턴\n",
    "        return graph\n",
    "\n",
    "# (GCN)Linear layer를 쓰냐 (GIN)MLP를 쓰냐만 차이임.\n",
    "class GraphIsomorphism(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            hidden_dim,\n",
    "            act=F.relu,\n",
    "            bias_mlp=True,\n",
    "            dropout_prob=0.2,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        \n",
    "\t\t# non-linear transformation 하기위해 [MLP] 선언\n",
    "        self.mlp = MLP(\n",
    "            input_dim=hidden_dim,\n",
    "            hidden_dim=4*hidden_dim,\n",
    "            output_dim=hidden_dim,\n",
    "            bias=bias_mlp,\n",
    "            act=act\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        self.prob = dropout_prob\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            graph, \n",
    "            training=False\n",
    "        ):\n",
    "        h0 = graph.ndata['h']\n",
    "\n",
    "        graph.update_all(fn.copy_u('h', 'm'), fn.sum('m', 'u_'))\n",
    "\t\t# aggregation된 message를 Linear layer가 아니라 [MLP]에 넣음\n",
    "        h = self.mlp(graph.ndata['u_']) + h0\n",
    "        h = self.norm(h)\n",
    "        \t\n",
    "        # Apply dropout on node features\n",
    "        h = F.dropout(h, p=self.prob, training=training)\n",
    "\n",
    "        graph.ndata['h'] = h\n",
    "        return graph\n",
    "        \n",
    "            \n",
    "# GIN_E: message를 전달할 때 neighbor node feature들만 받을 수도 있지만, 어떤 edge feature를 통해서 aggregation 되었나도 같이 고려할 수 있음.\n",
    "class GraphIsomorphismEdge(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            hidden_dim,\n",
    "            act=F.relu,\n",
    "            bias_mlp=True,\n",
    "            dropout_prob=0.2,\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        self.prob = dropout_prob\n",
    "\t\t# non-linear transformation 하기위해 [MLP] 선언\n",
    "        self.mlp = MLP(\n",
    "            input_dim=hidden_dim,\n",
    "            hidden_dim=4*hidden_dim,\n",
    "            output_dim=hidden_dim,\n",
    "            bias=bias_mlp,\n",
    "            act=act,\n",
    "        )\n",
    "        \n",
    "    def forward(\n",
    "            self, \n",
    "            graph, \n",
    "            training=False\n",
    "        ):\n",
    "        h0 = graph.ndata['h']\n",
    "\n",
    "\t\t# initial node feature들(h0)을 m에 copy해서 neighbor node feature들을 sum aggregation한 것을 neigh라고 하자\n",
    "        graph.update_all(fn.copy_u('h', 'm'), fn.sum('m', 'neigh'))\n",
    "\t\t# 예를들어 center node에 달린 edge의 edge feature들을 전부 copy후 sum(:sum한게 message에 들어오는 것들임)\n",
    "        graph.update_all(fn.copy_edge('e_ij', 'm_e'), fn.sum('m_e', 'u_'))\n",
    "\t\t# 주변의 aggregation node feature들 +  aggregation한 edge feature들 (concat한 것과 같은 효과)\n",
    "        # 주변의 node feature들만 aggregation한 게 아니라 주변 neighbor들에 달려있는 edge, 그 edge가 가지고 있는 edge feature들 까지 같이 sum aggregaton한 것\n",
    "        u_ = graph.ndata['neigh'] + graph.ndata['u_']\n",
    "        # aggregation된 message를 non-linear transformation[MLP]하여 residual conncection(원래 node feature를 더함)\n",
    "        h = self.mlp(u_) + h0\n",
    "        h = self.norm(h)\n",
    "        \n",
    "        # Apply dropout on node features\n",
    "        h = F.dropout(h, p=self.prob, training=training)\n",
    "\n",
    "        graph.ndata['h'] = h\n",
    "        return graph\n",
    "\n",
    "# 이론 PPT 112p\n",
    "class GraphAttention(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            hidden_dim,\n",
    "            num_heads=4,\n",
    "            bias_mlp=True,\n",
    "            dropout_prob=0.2,\n",
    "            act=F.relu,\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mlp = MLP(\n",
    "            input_dim=hidden_dim,\n",
    "            hidden_dim=2*hidden_dim,\n",
    "            output_dim=hidden_dim,\n",
    "            bias=bias_mlp,\n",
    "            act=act,\n",
    "        )\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        # multi head dimension을 하되 맨처음의 hidden dimension과 맨 나중의 output dimension이 같도록\n",
    "        # attention의 head는 num_heads니까 각 head에서 계산이 되는 dimension이 원래 hidden dimention 나누기 num_head(head의 개수)가 되어야함.\n",
    "        self.splitted_dim = hidden_dim // num_heads\n",
    "\n",
    "        self.prob = dropout_prob\n",
    "\n",
    "        # linear transformation을 하되 bias는 쓰지 않음\n",
    "        self.w1 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.w3 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.w4 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.w5 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.w6 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "\n",
    "        self.act = F.elu\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "            \n",
    "    def forward(\n",
    "            self, \n",
    "            graph, \n",
    "            # 밑에 dropout이 있어서 써놓음. training phase면 True. dropout을 킴.\n",
    "            training=False\n",
    "        ):\n",
    "        # h0: 처음 graph의 각 node들의 data\n",
    "        h0 = graph.ndata['h']\n",
    "        e_ij = graph.edata['e_ij']\n",
    "        \n",
    "\t\t### ai,j 구하는 과정\n",
    "        # u: source node, v: neighbor node, x_ij: edge feature\n",
    "        # <w1(h0)> source node에 linear transformation \n",
    "        # -1은 batch size 의미.\n",
    "        graph.ndata['u'] = self.w1(h0).view(-1, self.num_heads, self.splitted_dim)\n",
    "        # <w2(h0)> neighbor node에 linear transformation\n",
    "        graph.ndata['v'] = self.w2(h0).view(-1, self.num_heads, self.splitted_dim)\n",
    "        # edge feature의 dimension은 원래 6인데, 위의 graph.ndata['v']와 더하려면 dimension이 같아야 하므로 \n",
    "        # <w3(e_ij)> edge feature에 linear tramsformation으로 dimension을 바꿔주고 \n",
    "        graph.edata['x_ij'] = self.w3(e_ij).view(-1, self.num_heads, self.splitted_dim)\n",
    "\n",
    "        # neighbor node 'v'에다가 edge feature 'x_ij'를 더해서 결과물을 'm'(message)이라고 하고 이 연산을 edge에다가 apply 하라.\n",
    "        graph.apply_edges(fn.v_add_e('v', 'x_ij', 'm'))\n",
    "        # source node 'u'와 'm'을 내적해서 결과물을 'attn'(attention)이라고 하고 이 연산을 edge에다가 apply 하라. \n",
    "        graph.apply_edges(fn.u_mul_e('u', 'm', 'attn'))\n",
    "        # 이 'attn' edge data를 루트 d로 나눠 scaling하고 softmax 함\n",
    "        graph.edata['attn'] = edge_softmax(graph, graph.edata['attn'] / math.sqrt(self.splitted_dim))\n",
    "        \n",
    "\n",
    "        ### xi' 구하는 과정\n",
    "        # <w4(h0)> 원래의 neighbor node에 linear transformation 한 것\n",
    "        graph.ndata['k'] = self.w4(h0).view(-1, self.num_heads, self.splitted_dim)\n",
    "        # <w5(e_ij)> 원래의 edge feature에 linear tramsformation 한 것\n",
    "        graph.edata['x_ij'] = self.w5(e_ij).view(-1, self.num_heads, self.splitted_dim)\n",
    "        # neighbor node 'k'에다가 edge feature 'x_ij'를 더해서 결과물을 'm'(message)이라고 하고 이 연산을 edge에다가 apply 하라.\n",
    "        graph.apply_edges(fn.v_add_e('k', 'x_ij', 'm'))\n",
    "\n",
    "        # 'attn'과 'm'을 각각 곱해서 더하는 linear combination 과정\n",
    "        # 'attn' 과 'm'을 각각 곱한 것을 다시 'm'이라고 하고\n",
    "        graph.edata['m'] = graph.edata['attn'] * graph.edata['m']\n",
    "        # 모든 edge 'm'을 copy하여 전부 더하여 'h'로 반환한 후 모두 업데이트하여라.\n",
    "        graph.update_all(fn.copy_edge('m', 'm'), fn.sum('m', 'h'))\n",
    "        \n",
    "        # 모든 업데이트한 데이터들을 \n",
    "        # multi-head attention으로 인해 self.splitted_dim으로 dimesion을 쪼갰던 것을 다시 self.hidden_dim으로 합쳐주고 \n",
    "        # 원래 node featrue에다가 linear transformation 한 것에 더해줌.\n",
    "        h = self.w6(h0) + graph.ndata['h'].view(-1, self.hidden_dim)\n",
    "        h = self.norm(h)\n",
    "\n",
    "\t\t# Add and Norm module\n",
    "\t\th = h + self.mlp(h)\n",
    "\t\th = self.norm(h)\n",
    "\t\t\t\n",
    "\t\t# Apply dropout on node features\n",
    "\t\th = F.dropout(h, p=self.prob, training=training)\n",
    "\n",
    "\t\tgraph.ndata['h'] = h \n",
    "\t\treturn graph\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
